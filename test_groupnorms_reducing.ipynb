{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing how PyTorch Group Normalization layers compute mean & variance (whether they collapse image dimensions or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom function for group normalization\n",
    "def group_norm2(input, num_groups, weight=None, bias=None, eps=1e-05, reduce=True,verbose=True):\n",
    "\n",
    "    batch_size, num_channels, height, width = input.size() \n",
    "    if verbose:\n",
    "        print(f'input size (BCHW): [{input.shape}]')\n",
    "\n",
    "    input = input.view(batch_size, num_groups, num_channels//num_groups, height, width)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'after grouping: [{input.shape}]')\n",
    "\n",
    "    if reduce:\n",
    "        mean = torch.mean(input, dim=(2,3,4), keepdim=True)\n",
    "        var = torch.var(input, dim=(2,3,4), unbiased=False, keepdim=True)\n",
    "        if verbose:\n",
    "            print('mean/var shape: ',mean.shape)\n",
    "    else:\n",
    "        mean = torch.mean(input, dim=2, keepdim=True)\n",
    "        var = torch.var(input, dim=2, unbiased=False, keepdim=True)\n",
    "        if verbose:\n",
    "            print('mean/var shape: ',mean.shape)\n",
    "    \n",
    "    input = (input - mean) / torch.sqrt(var+eps)\n",
    "\n",
    "    input = input.view(batch_size, num_channels, height, width)\n",
    "\n",
    "    if weight is None:\n",
    "        weight = torch.tensor([1]).repeat(num_channels)\n",
    "    if bias is None:\n",
    "        bias = torch.tensor([0]).repeat(num_channels)\n",
    "\n",
    "    # transform weight & bias [C] -> [N, C, H, W]\n",
    "    weight = weight.unsqueeze(0).unsqueeze(2).unsqueeze(3) # [1, C, 1, 1]\n",
    "    weight = weight.expand(batch_size, num_channels, height, width) # [N, C, H, W]\n",
    "    bias = bias.unsqueeze(0).unsqueeze(2).unsqueeze(3) # [1, C, 1, 1]\n",
    "    bias = bias.expand(batch_size, num_channels, height, width) # [N, C, H, W]\n",
    "\n",
    "    input = input*weight + bias \n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group norm reducing H & W dimensions\n",
      "input size (BCHW): [torch.Size([1, 6, 3, 3])]\n",
      "after grouping: [torch.Size([1, 3, 2, 3, 3])]\n",
      "mean/var shape:  torch.Size([1, 3, 1, 1, 1])\n",
      "\n",
      "Group norm without reducing H & W dimensions\n",
      "input size (BCHW): [torch.Size([1, 6, 3, 3])]\n",
      "after grouping: [torch.Size([1, 3, 2, 3, 3])]\n",
      "mean/var shape:  torch.Size([1, 3, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "N, C, H, W = 1, 6, 3, 3 # batch size, channel, height, width\n",
    "\n",
    "n_groups = 3\n",
    "\n",
    "input = torch.randn(N,C,H,W)\n",
    "\n",
    "# random weights\n",
    "gamma = torch.randn(C)\n",
    "beta = torch.randn(C)\n",
    "\n",
    "out =group_norm = torch.nn.functional.group_norm(\n",
    "    input,\n",
    "    num_groups = n_groups,\n",
    "    weight = gamma,\n",
    "    bias= beta,\n",
    "    eps = 1e-05\n",
    ")\n",
    "\n",
    "print('Group norm reducing H & W dimensions')\n",
    "out2= group_norm2(input, num_groups= n_groups, weight = gamma, bias = beta, eps=1e-05)\n",
    "\n",
    "print('\\nGroup norm without reducing H & W dimensions')\n",
    "out3= group_norm2(input, num_groups= n_groups, weight = gamma, bias = beta, eps=1e-05, reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch group norm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.9086,  0.5706,  0.3820,  0.9110, -1.0005,  1.7645, -0.3582, -0.3094,\n",
       "        -1.4918,  1.7138,  1.6224,  1.4765,  0.7816,  1.8332,  1.9592,  1.9472,\n",
       "         0.7319,  1.8505,  0.1094,  0.1917,  0.3087,  0.3940,  0.2060,  0.1634,\n",
       "         0.3426,  0.2738,  0.3738, -1.4001, -1.2208, -0.4686, -1.0652, -0.6814,\n",
       "        -0.5616, -1.3275, -0.5260, -0.0893, -0.5445, -0.4432, -0.5275, -0.5387,\n",
       "        -0.6562, -0.4527, -0.5573, -0.5553, -0.5849, -1.0185, -0.8639, -0.6323,\n",
       "        -0.9397, -0.9468, -1.1695, -0.9483, -0.6847, -0.9567])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group norm (computes mean/var by collapsing H & W dimensions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.9086,  0.5706,  0.3820,  0.9110, -1.0005,  1.7645, -0.3582, -0.3094,\n",
       "        -1.4918,  1.7138,  1.6224,  1.4765,  0.7816,  1.8332,  1.9592,  1.9472,\n",
       "         0.7319,  1.8505,  0.1094,  0.1917,  0.3087,  0.3940,  0.2060,  0.1634,\n",
       "         0.3426,  0.2738,  0.3738, -1.4001, -1.2208, -0.4686, -1.0652, -0.6814,\n",
       "        -0.5616, -1.3275, -0.5260, -0.0893, -0.5445, -0.4432, -0.5275, -0.5387,\n",
       "        -0.6562, -0.4527, -0.5573, -0.5553, -0.5849, -1.0185, -0.8639, -0.6323,\n",
       "        -0.9397, -0.9468, -1.1695, -0.9483, -0.6847, -0.9567])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group norm (computes mean/var only along C dimensions (H & W dimensions are not collapsed))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.0115,  1.1384,  1.1107, -1.0115, -1.0114,  1.1384,  1.1383, -1.0115,\n",
       "        -1.0115,  1.1727,  2.0070,  1.9963,  1.1727,  1.1727,  2.0071,  2.0070,\n",
       "         1.1727,  1.1727,  0.1224,  0.1224,  0.3235,  0.3235,  0.3235,  0.3235,\n",
       "         0.3235,  0.3235,  0.3235, -1.4775, -1.4774, -0.5281, -0.5281, -0.5282,\n",
       "        -0.5282, -0.5282, -0.5281, -0.5281, -0.5867, -0.4514, -0.4514, -0.5867,\n",
       "        -0.5867, -0.5867, -0.5867, -0.4514, -0.5867, -1.0133, -0.7042, -0.7042,\n",
       "        -1.0133, -1.0133, -1.0133, -1.0133, -0.7042, -1.0133])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output differences: \n",
      "Between pytorch group norm and the custom group norm with H&W collapsing: -1.1175870895385742e-07\n",
      "Between pytorch group norm and the custom group norm without H&W collapsing: -0.6618859767913818\n"
     ]
    }
   ],
   "source": [
    "print('pytorch group norm')\n",
    "display(out.reshape((-1)))\n",
    "\n",
    "print('group norm (computes mean/var by collapsing H & W dimensions)')\n",
    "display(out2.reshape((-1)))\n",
    "\n",
    "print('group norm (computes mean/var only along C dimensions (H & W dimensions are not collapsed))')\n",
    "display(out3.reshape((-1)))\n",
    "\n",
    "print('Output differences: ')\n",
    "print(f'Between pytorch group norm and the custom group norm with H&W collapsing: {(out-out2).sum()}')\n",
    "print(f'Between pytorch group norm and the custom group norm without H&W collapsing: {(out-out3).sum()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
