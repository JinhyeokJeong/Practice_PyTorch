{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing how PyTorch Group Normalization layers compute mean & variance (whether they collapse image dimensions or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom function for group normalization\n",
    "def group_norm2(input, num_groups, weight=None, bias=None, eps=1e-05, reduce=True,verbose=True):\n",
    "\n",
    "    batch_size, num_channels, height, width = input.size() \n",
    "    if verbose:\n",
    "        print(f'input size (BCHW): [{input.shape}]')\n",
    "\n",
    "    input = input.view(batch_size, num_groups, num_channels//num_groups, height, width)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'after grouping: [{input.shape}]')\n",
    "\n",
    "    if reduce:\n",
    "        mean = torch.mean(input, dim=(2,3,4), keepdim=True)\n",
    "        var = torch.var(input, dim=(2,3,4), unbiased=False, keepdim=True)\n",
    "        if verbose:\n",
    "            print('mean/var shape: ',mean.shape)\n",
    "    else:\n",
    "        mean = torch.mean(input, dim=2, keepdim=True)\n",
    "        var = torch.var(input, dim=2, unbiased=False, keepdim=True)\n",
    "        if verbose:\n",
    "            print('mean/var shape: ',mean.shape)\n",
    "    \n",
    "    input = (input - mean) / torch.sqrt(var+eps)\n",
    "\n",
    "    input = input.view(batch_size, num_channels, height, width)\n",
    "\n",
    "    if weight is None:\n",
    "        weight = torch.tensor([1]).repeat(num_channels)\n",
    "    if bias is None:\n",
    "        bias = torch.tensor([0]).repeat(num_channels)\n",
    "\n",
    "    # transform weight & bias [C] -> [N, C, H, W]\n",
    "    weight = weight.unsqueeze(0).unsqueeze(2).unsqueeze(3) # [1, C, 1, 1]\n",
    "    weight = weight.expand(batch_size, num_channels, height, width) # [N, C, H, W]\n",
    "    bias = bias.unsqueeze(0).unsqueeze(2).unsqueeze(3) # [1, C, 1, 1]\n",
    "    bias = bias.expand(batch_size, num_channels, height, width) # [N, C, H, W]\n",
    "\n",
    "    input = input*weight + bias \n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group norm reducing H & W dimensions\n",
      "input size (BCHW): [torch.Size([1, 6, 3, 3])]\n",
      "after grouping: [torch.Size([1, 3, 2, 3, 3])]\n",
      "mean/var shape:  torch.Size([1, 3, 1, 1, 1])\n",
      "\n",
      "Group norm without reducing H & W dimensions\n",
      "input size (BCHW): [torch.Size([1, 6, 3, 3])]\n",
      "after grouping: [torch.Size([1, 3, 2, 3, 3])]\n",
      "mean/var shape:  torch.Size([1, 3, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "N, C, H, W = 1, 6, 3, 3 # batch size, channel, height, width\n",
    "\n",
    "n_groups = 3\n",
    "\n",
    "input = torch.randn(N,C,H,W)\n",
    "\n",
    "# random weights\n",
    "gamma = torch.randn(C)\n",
    "beta = torch.randn(C)\n",
    "\n",
    "out =group_norm = torch.nn.functional.group_norm(\n",
    "    input,\n",
    "    num_groups = n_groups,\n",
    "    weight = gamma,\n",
    "    bias= beta,\n",
    "    eps = 1e-05\n",
    ")\n",
    "\n",
    "print('Group norm reducing H & W dimensions')\n",
    "out2= group_norm2(input, num_groups= n_groups, weight = gamma, bias = beta, eps=1e-05)\n",
    "\n",
    "print('\\nGroup norm without reducing H & W dimensions')\n",
    "out3= group_norm2(input, num_groups= n_groups, weight = gamma, bias = beta, eps=1e-05, reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch group norm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.8982e-01, -1.0988e+00,  1.5307e+00, -7.2852e-01,  1.9463e+00,\n",
       "        -1.8436e+00, -2.6760e-01,  2.0959e+00, -1.6022e+00,  2.3373e-01,\n",
       "        -1.2876e-01, -1.7684e-02,  5.6397e-02, -1.5088e-01, -8.1340e-01,\n",
       "        -5.0214e-01, -2.1661e+00, -5.2516e-01, -1.0343e+00, -1.3529e-02,\n",
       "        -2.4171e-01, -4.0385e-01,  3.7268e-02, -5.0453e-01,  2.8989e-01,\n",
       "         5.3631e-02, -2.9229e-01,  2.5705e+00,  8.6528e-01,  2.4461e+00,\n",
       "         4.0048e-02, -4.1047e-01, -3.0143e-01,  2.0454e+00,  7.0477e-02,\n",
       "         1.2665e+00,  1.3012e+00,  3.4624e-01,  3.5497e+00, -5.6892e-01,\n",
       "         1.3078e+00,  2.7435e-01,  1.2711e-01, -1.0073e+00,  9.0564e-01,\n",
       "         3.1841e+00, -5.2309e-01, -1.8582e+00, -3.5025e+00,  2.7110e-01,\n",
       "         1.7705e+00,  3.2191e-03,  1.9183e+00,  6.7090e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group norm (computes mean/var by collapsing H & W dimensions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.8982e-01, -1.0988e+00,  1.5307e+00, -7.2852e-01,  1.9463e+00,\n",
       "        -1.8436e+00, -2.6760e-01,  2.0959e+00, -1.6022e+00,  2.3373e-01,\n",
       "        -1.2876e-01, -1.7684e-02,  5.6397e-02, -1.5088e-01, -8.1340e-01,\n",
       "        -5.0214e-01, -2.1661e+00, -5.2516e-01, -1.0343e+00, -1.3529e-02,\n",
       "        -2.4171e-01, -4.0385e-01,  3.7268e-02, -5.0453e-01,  2.8989e-01,\n",
       "         5.3631e-02, -2.9229e-01,  2.5705e+00,  8.6528e-01,  2.4461e+00,\n",
       "         4.0048e-02, -4.1047e-01, -3.0143e-01,  2.0454e+00,  7.0477e-02,\n",
       "         1.2665e+00,  1.3012e+00,  3.4624e-01,  3.5497e+00, -5.6892e-01,\n",
       "         1.3078e+00,  2.7435e-01,  1.2711e-01, -1.0073e+00,  9.0564e-01,\n",
       "         3.1841e+00, -5.2309e-01, -1.8582e+00, -3.5025e+00,  2.7110e-01,\n",
       "         1.7705e+00,  3.2190e-03,  1.9183e+00,  6.7090e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group norm (computes mean/var only along C dimensions (H & W dimensions are not collapsed))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6560,  1.6541,  1.6560,  1.6559,  1.6560, -1.9822, -1.9426, -1.9822,\n",
       "        -1.9822,  0.0700,  0.0694,  0.0700,  0.0700,  0.0700, -1.1048, -1.0920,\n",
       "        -1.1048, -1.1048, -0.4418, -0.4417, -0.4418, -0.4418,  0.3042, -0.4418,\n",
       "        -0.4417,  0.3041, -0.4418,  1.7377,  1.7372,  1.7377,  1.7376, -1.0781,\n",
       "         1.7376,  1.7373, -1.0781,  1.7377, -1.0334,  1.7502,  1.7502,  1.7502,\n",
       "         1.7502, -1.0334,  1.7494, -1.0334,  1.7500,  2.5037, -1.1869, -1.1870,\n",
       "        -1.1870, -1.1870,  2.5037, -1.1859,  2.5038, -1.1868])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output differences: \n",
      "Between pytorch group norm and the custom group norm with H&W collapsing: 0.00000\n",
      "Between pytorch group norm and the custom group norm without H&W collapsing: -0.48400\n"
     ]
    }
   ],
   "source": [
    "print('pytorch group norm')\n",
    "display(out.reshape((-1)))\n",
    "\n",
    "print('group norm (computes mean/var by collapsing H & W dimensions)')\n",
    "display(out2.reshape((-1)))\n",
    "\n",
    "print('group norm (computes mean/var only along C dimensions (H & W dimensions are not collapsed))')\n",
    "display(out3.reshape((-1)))\n",
    "\n",
    "print('Output differences: ')\n",
    "print(f'Between pytorch group norm and the custom group norm with H&W collapsing: {(out-out2).sum():.5f}')\n",
    "print(f'Between pytorch group norm and the custom group norm without H&W collapsing: {(out-out3).sum():.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
